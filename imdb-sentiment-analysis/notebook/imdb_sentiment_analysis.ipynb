{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis on the IMDB dataset using an LSTM model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Load the IMDB review dataset with metadata\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "\n",
        "# Convert TensorFlow tensors to Python strings and integers\n",
        "reviews = []\n",
        "sentiments = []\n",
        "for review, sentiment in dataset['train']:\n",
        "    reviews.append(review.numpy().decode('utf-8'))\n",
        "    sentiments.append(sentiment.numpy())\n",
        "\n",
        "# Save the dataset as a CSV file\n",
        "df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
        "df.to_csv('IMDB_dataset.csv', index=False)\n",
        "\n",
        "# Load the dataset and split into features and labels\n",
        "df = pd.read_csv(\"IMDB_dataset.csv\")\n",
        "X = df['review'].values\n",
        "y = df['sentiment'].values\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Tokenize and pad sequences using Keras Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "# Plot histogram of sentence lengths\n",
        "plt.hist([len(s) for s in train_sequences] + [len(s) for s in test_sequences], bins=50)\n",
        "plt.show()\n",
        "\n",
        "# Set max sequence length and apply padding\n",
        "max_length = 300\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating='post', padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, truncating='post', padding='post')\n",
        "\n",
        "# Check reverse conversion from sequences to words\n",
        "text = train_padded[0]\n",
        "print(' '.join([tokenizer.index_word.get(i, '<pad>') for i in text]))\n",
        "print(train_sentences[0])\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "train_data = TensorDataset(torch.LongTensor(train_padded), torch.FloatTensor(train_labels))\n",
        "valid_data = TensorDataset(torch.LongTensor(test_padded), torch.FloatTensor(test_labels))\n",
        "\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Define LSTM model class\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, no_layers, vocab_size, embedding_dim, hidden_dim, output_dim, drop_prob=0.3):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,\n",
        "                            num_layers=no_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sig_out = self.sig(out)\n",
        "        return sig_out[:, -1], hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
        "        return (h0, c0)\n",
        "\n",
        "# Instantiate the model and move to device\n",
        "no_layers = 1\n",
        "embedding_dim = 64\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "model = SentimentRNN(no_layers, vocab_size + 1, embedding_dim, hidden_dim, output_dim, drop_prob=0.3)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss, optimizer, and accuracy function\n",
        "lr = 0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def acc(pred, label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()\n",
        "\n",
        "# Training loop\n",
        "import time\n",
        "s = time.time()\n",
        "clip = 5\n",
        "epochs = 10\n",
        "valid_loss_min = np.inf\n",
        "\n",
        "LOSS, VAL_LOSS, ACC, VAL_ACC = [], [], [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        h = model.init_hidden(batch_size)\n",
        "        output, hidden = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc(output, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_data)\n",
        "    train_acc /= len(train_data)\n",
        "    LOSS.append(train_loss)\n",
        "    ACC.append(train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_acc = 0, 0\n",
        "    with torch.inference_mode():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            h = model.init_hidden(batch_size)\n",
        "            output, hidden = model(inputs, h)\n",
        "            loss = criterion(output.squeeze(), labels)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += acc(output, labels)\n",
        "\n",
        "    val_loss /= len(valid_data)\n",
        "    val_acc /= len(valid_data)\n",
        "    VAL_LOSS.append(val_loss)\n",
        "    VAL_ACC.append(val_acc)\n",
        "\n",
        "    print(f'epoch {epoch} ==> train loss: {train_loss:.5f},  validation loss: {val_loss:.5f}',\n",
        "          f'train acc: {train_acc:.5f}, validation acc: {val_acc:.5f}')\n",
        "\n",
        "    if val_loss <= valid_loss_min:\n",
        "        torch.save(model.state_dict(), 'state_dict.pt')\n",
        "        print('model saved.............')\n",
        "        valid_loss_min = val_loss\n",
        "\n",
        "print((time.time() - s)/60)\n",
        "\n",
        "fig = plt.figure(figsize = (20, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ACC, label='Train')\n",
        "plt.plot(VAL_ACC, label='Validation')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(LOSS, label='Train')\n",
        "plt.plot(VAL_LOSS, label='Validation')\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Inference function\n",
        "def predict_text(text):\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, truncating='post', padding='post')\n",
        "    inputs = torch.tensor(padded).to(device)\n",
        "    h = model.init_hidden(1)\n",
        "    output, hidden = model(inputs, h)\n",
        "    return output.item()\n",
        "\n",
        "# Test example sentences\n",
        "texts = [\n",
        "    \"The storyline was predictable, but the acting saved the film.\",\n",
        "    \"Absolutely terrible. I want my two hours back.\",\n",
        "    \"One of the best movies I’ve seen this year – touching and inspiring.\"\n",
        "]\n",
        "\n",
        "for t in texts:\n",
        "    pred = predict_text(t)\n",
        "    print(f\"Text: {t}\")\n",
        "    print(f\"Predicted: {'positive' if pred > 0.5 else 'negative'} (score: {pred:.4f})\\n\")"
      ],
      "metadata": {
        "id": "ZnpwjOT3j-lZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}